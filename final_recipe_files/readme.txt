*Managed by Melissa Merz*

recipe_scraper.py -> Runs in the terminal the same way that crawler.py did in pa2. Scrapes all the recipe data off of allrecipes.com which match certain criteria, namely that they don't have any categories in the BAD_CATS list, and that they have at least 15 ratings with an average rating of at least 3.5/5. Has to visit each page, as the information needed to check these criteria is buried in the html. Works on both types of recipes on the site, but has to do a find_all for all the potential tag types of both, as they are hard to distinguish efficiently, and adding a few dozen or so extra tags to loop over isn't very computationally expensive. All the scraped information is then added to the 4 csvs: one containing the general recipe info like its rating and number of servings, one containing the information about each recipes ingredients (amount and units), one containing the categories for each recipe, and finally one mapping ingredient unique ids to ingredient names.

final_recipes.csv -> the final, best version of the recipes.csv file, containing details for 17944 recipes. It has 10 columns, which are recipe_num (unique identifier for each recipe) which serves as the index, title (name of recipe), url (link to recipe), image (link to the "best" image of the recipe), servings (number of servings the recipe makes by default), prep_time (how long it takes to prepare the ingredients to cook), cook_time (how long it takes to actually cook the recipe), total_time (how long, in total, it takes to make the recipe, including things like chilling it in the fridge, or allowing it to rest), n_reviews (how many reviews the recipe has received), and avg_review (the average score out of 5 given to the recipe by the reviewers). Several of these quantities can potentially come back as None, which just means that the user who created the recipe either didn't include it, or it couldn't be found by the scraper (though in the case of these recipes it should always be for the first reason).

final_recipe_ingredients.csv -> the final, best version of the recipe_ingredients.csv file, containing details for all 17944 recipes from final_recipes.csv. It has 4 columns, which are recipe_num (the unique identifier for the recipe which this row is attached to), amount (the numerical component of the amount required by the recipe), unit (the unit component of the amount required by the recipe -- like cups or tablespoons), and ingredient_id (the associated unique ingredient identifier for this ingredient -- can be joined with the final_ingredient_codes.csv file to find the name of the ingredient in a given row). Like with final_recipes.csv, these values can come back as NaN and None, for the amount and units respectively, usually due to the ingredient not having either of them, such as when a recipe calls for "salt and pepper to taste", which has neither units nor and amount associated with it.

final_recipe_categories.csv -> the final, best version of the recipe_categories.csv file, containing details for all 17944 recipes from final_recipes.csv. It has just 2 columns, recipe_num (the unique identifier for the recipe which this row is attached to), and category (the name of a given category for a given recipe_num).

final_ingredient_codes.csv -> the final, best version of the ingredient_codes.csv file, containing details for all the ingredients found in the 17944 recipes from final_recipes.csv. It has just 2 columns, ingredient_id (the associated unique ingredient identifier for this ingredient), and name (the shortest name of the ingredient found in all 17944 recipes -- this helps make matching using Jaro-Winkler easier later on, since "chicken breast" is easier to match to than "chicken breast, cut in 1-inch cubes").
